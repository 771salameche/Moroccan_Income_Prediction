{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon May 12 23:05:16 2025\n",
    "\n",
    "@author: salah\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Jupyter Notebook for Data Preprocessing - Moroccan Income Prediction\n",
    "\n",
    "This notebook details the steps taken to preprocess the raw dataset for the\n",
    "Moroccan Income Prediction project. The goal is to clean and transform the\n",
    "data into a suitable format for machine learning modeling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5595b1",
   "metadata": {},
   "source": [
    "# Moroccan Income Prediction: Data Preprocessing\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### Purpose\n",
    "Data preprocessing is a crucial step in the machine learning pipeline. Raw data is often messy, containing missing values, outliers, and features in formats unsuitable for modeling algorithms (e.g., categorical text). This notebook aims to address these issues by cleaning, transforming, and structuring the data. Effective preprocessing improves model performance, stability, and interpretability.\n",
    "\n",
    "### Techniques Overview\n",
    "We will apply a series of standard and domain-specific preprocessing techniques:\n",
    "1.  **Data Loading & Initial Assessment:** Load the dataset and review findings from the Exploratory Data Analysis (EDA) phase.\n",
    "2.  **Missing Data Handling:** Analyze missing data patterns and apply appropriate imputation strategies (e.g., mean, median, mode, or more advanced methods like KNN imputation).\n",
    "3.  **Outlier Treatment:** Detect outliers using statistical methods (e.g., IQR, Z-score) or visualization and apply treatment strategies like capping, transformation, or removal.\n",
    "4.  **Feature Encoding:** Convert categorical features into numerical representations using One-Hot Encoding, Label Encoding, or Ordinal Encoding based on the nature of the variable.\n",
    "5.  **Feature Scaling:** Standardize or normalize numerical features to bring them onto a common scale, preventing features with larger values from dominating the model.\n",
    "6.  **Feature Engineering:** Create new features from existing ones (e.g., interaction terms, polynomial features, binning) to potentially capture more complex relationships.\n",
    "7.  **Dimensionality Assessment:** Analyze correlations and multicollinearity, potentially applying techniques like Principal Component Analysis (PCA) for dimensionality reduction if needed.\n",
    "8.  **Pipeline Creation:** Build a reproducible preprocessing pipeline using `scikit-learn`'s `Pipeline` and `ColumnTransformer` to streamline the process and prevent data leakage.\n",
    "\n",
    "### Expected Outcomes\n",
    "-   Cleaned and transformed training and testing datasets.\n",
    "-   A serialized `scikit-learn` preprocessing pipeline object that can be applied consistently to new data (e.g., during prediction).\n",
    "-   Documentation of all preprocessing steps and decisions made.\n",
    "-   Data ready for the modeling phase.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf2abe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder,\n",
    "    OrdinalEncoder,\n",
    "    PowerTransformer\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Configure visualization style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Suppress specific warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac67dd",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Assessment\n",
    "\n",
    "### 2.1 Load Raw Data\n",
    "We start by loading the raw dataset. **Important:** Replace `'path/to/your/moroccan_income_raw.csv'` with the actual path to your data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0a77d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- !!! ACTION REQUIRED !!! ---\n",
    "# Replace with the correct path to your dataset\n",
    "DATA_PATH = 'moroccan_income_raw.csv'\n",
    "# ---\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    df_raw = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Dataset loaded successfully from {DATA_PATH}\")\n",
    "    print(f\"Shape of the raw dataset: {df_raw.shape}\")\n",
    "    print(\"First 5 rows of the raw dataset:\")\n",
    "    display(df_raw.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {DATA_PATH} was not found.\")\n",
    "    print(\"Please ensure the file path is correct.\")\n",
    "    # As a fallback for demonstration, create a dummy dataframe\n",
    "    print(\"Creating a dummy DataFrame for demonstration purposes...\")\n",
    "    dummy_data = {\n",
    "        'ID': range(1, 101),\n",
    "        'Age': np.random.randint(18, 70, 100),\n",
    "        'Categorie_Age': ['Adulte']*50 + ['Senior']*30 + ['Jeune Adulte']*20,\n",
    "        'Sexe': np.random.choice(['Homme', 'Femme'], 100),\n",
    "        'Milieu': np.random.choice(['Urbain', 'Rural'], 100),\n",
    "        'Niveau_Education': np.random.choice(['Primaire', 'Secondaire', 'Universitaire', 'Non scolarisé', None], 100, p=[0.2, 0.3, 0.3, 0.1, 0.1]),\n",
    "        'Annees_Experience': np.random.randint(0, 40, 100),\n",
    "        'Etat_Matrimonial': np.random.choice(['Célibataire', 'Marié(e)', 'Divorcé(e)', 'Veuf(ve)'], 100),\n",
    "        'Categorie_Socioprofessionnelle': np.random.choice(['Employé', 'Ouvrier', 'Cadre', 'Indépendant', 'Sans emploi', None], 100, p=[0.3, 0.2, 0.15, 0.15, 0.1, 0.1]),\n",
    "        'Possession_Voiture': np.random.choice([True, False, None], 100, p=[0.3, 0.6, 0.1]),\n",
    "        'Possession_Logement': np.random.choice([True, False], 100),\n",
    "        'Possession_Terrain': np.random.choice([True, False], 100),\n",
    "        'Nb_Personnes_Charge': np.random.randint(0, 6, 100),\n",
    "        'Secteur_Activite': np.random.choice(['Agriculture', 'Industrie', 'Services', 'Administration', 'Autre', None], 100, p=[0.15, 0.2, 0.3, 0.15, 0.1, 0.1]),\n",
    "        'Acces_Internet': np.random.choice([True, False], 100),\n",
    "        'Revenu_Annuel': np.random.gamma(2, 50000, 100) + 10000, # Target variable\n",
    "        'Age_Mois': lambda x: x['Age'] * 12 + np.random.randint(0, 12, 100), # Example derived column\n",
    "        'Education_Superieure': lambda x: x['Niveau_Education'] == 'Universitaire', # Example derived column\n",
    "        'Indice_Richesse': np.random.rand(100) * 10,\n",
    "        'Couleur_Preferee': np.random.choice(['Bleu', 'Rouge', 'Vert', 'Jaune', 'Noir'], 100),\n",
    "        'Code_Postal': np.random.randint(10000, 99999, 100),\n",
    "        'ID_Aleatoire': [f'ID_{i}' for i in range(1000, 1100)]\n",
    "    }\n",
    "    df_raw = pd.DataFrame(dummy_data)\n",
    "    # Apply lambda functions if needed\n",
    "    for col, func in dummy_data.items():\n",
    "        if callable(func):\n",
    "            df_raw[col] = func(df_raw)\n",
    "    df_raw['Revenu_Annuel'].iloc[::10] = np.nan # Add some missing target values\n",
    "    df_raw['Annees_Experience'].iloc[::15] = np.nan # Add some missing feature values\n",
    "    print(\"Dummy DataFrame created:\")\n",
    "    display(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c397c000",
   "metadata": {},
   "source": [
    "### 2.2 Review EDA Findings & Identify Irrelevant Columns\n",
    "Based on the Exploratory Data Analysis (EDA) phase (assumed to be done previously), we summarize key findings and identify columns to drop.\n",
    "\n",
    "**Assumed EDA Findings (Examples - Replace with your actual findings):**\n",
    "* High correlation between `Age` and `Age_Mois`.\n",
    "* `Education_Superieure` seems derivable from `Niveau_Education`.\n",
    "* `ID`, `ID_Aleatoire`, `Code_Postal`, `Couleur_Preferee` are likely identifiers or irrelevant features for income prediction.\n",
    "* Missing values observed in `Niveau_Education`, `Categorie_Socioprofessionnelle`, `Possession_Voiture`, `Secteur_Activite`, `Annees_Experience`, and potentially `Revenu_Annuel`.\n",
    "* Potential outliers in `Revenu_Annuel` and `Annees_Experience`.\n",
    "* Skewness observed in `Revenu_Annuel`.\n",
    "\n",
    "**Columns to Drop:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81b49c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define columns to drop\n",
    "# ID columns, potentially redundant columns, high cardinality irrelevant columns\n",
    "columns_to_drop = ['ID', 'ID_Aleatoire', 'Code_Postal', 'Couleur_Preferee', 'Age_Mois']\n",
    "\n",
    "# Check if columns exist before dropping\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in df_raw.columns]\n",
    "\n",
    "if existing_columns_to_drop:\n",
    "    df = df_raw.drop(columns=existing_columns_to_drop)\n",
    "    print(f\"Dropped columns: {existing_columns_to_drop}\")\n",
    "    print(f\"Shape after dropping columns: {df.shape}\")\n",
    "else:\n",
    "    df = df_raw.copy()\n",
    "    print(\"No specified columns to drop were found.\")\n",
    "\n",
    "# Also, drop rows where the target variable is missing, as they cannot be used for training/evaluation.\n",
    "target_variable = 'Revenu_Annuel'\n",
    "initial_rows = df.shape[0]\n",
    "df.dropna(subset=[target_variable], inplace=True)\n",
    "rows_after_target_na_drop = df.shape[0]\n",
    "print(f\"Dropped {initial_rows - rows_after_target_na_drop} rows due to missing target variable '{target_variable}'.\")\n",
    "print(f\"Shape after dropping missing target rows: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb7cfd",
   "metadata": {},
   "source": [
    "### 2.3 Setup Logging\n",
    "We set up logging to keep track of the transformations applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd02740",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Logging setup complete.\")\n",
    "logging.info(f\"Initial DataFrame shape (after dropping irrelevant columns and target NaNs): {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d319ea9",
   "metadata": {},
   "source": [
    "### 2.4 Create Train/Test Split\n",
    "Split the data into training and testing sets *before* applying most preprocessing steps (especially imputation and scaling) to prevent data leakage from the test set into the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b5915",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=[target_variable])\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data (e.g., 80% train, 20% test)\n",
    "# Use stratify if dealing with classification or imbalanced regression if possible (e.g., by binning y)\n",
    "# For now, using random_state for reproducibility\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    logging.info(f\"Data split into training and testing sets.\")\n",
    "    logging.info(f\"Training set shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "    logging.info(f\"Testing set shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during train/test split: {e}\")\n",
    "    # Handle potential errors, e.g., if dataset is too small\n",
    "    if df.shape[0] < 5: # Example threshold\n",
    "         logging.warning(\"Dataset is very small, consider getting more data before splitting.\")\n",
    "         # Fallback: use the whole dataset for demonstration if split fails\n",
    "         X_train, X_test, y_train, y_test = X, X.copy(), y, y.copy() # Not ideal, just for notebook execution\n",
    "    else:\n",
    "         raise e # Re-raise other errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc26ff3c",
   "metadata": {},
   "source": [
    "## 3. Missing Data Handling\n",
    "\n",
    "### 3.1 Analysis of Missing Data\n",
    "Identify columns with missing values and the extent of missingness in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4da9a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Calculate missing values in the training set\n",
    "missing_values = X_train.isnull().sum()\n",
    "missing_percent = (missing_values / len(X_train)) * 100\n",
    "missing_summary = pd.DataFrame({'Missing Count': missing_values, 'Missing Percent (%)': missing_percent})\n",
    "missing_summary = missing_summary[missing_summary['Missing Count'] > 0].sort_values(by='Missing Percent (%)', ascending=False)\n",
    "\n",
    "print(\"Missing data summary (Training Set):\")\n",
    "if missing_summary.empty:\n",
    "    print(\"No missing values found in the training features.\")\n",
    "    logging.info(\"No missing values detected in the training features.\")\n",
    "else:\n",
    "    display(missing_summary)\n",
    "    logging.info(f\"Missing values identified in columns: {missing_summary.index.tolist()}\")\n",
    "\n",
    "    # Optional: Visualize missing data patterns (e.g., using missingno library)\n",
    "    try:\n",
    "        import missingno as msno\n",
    "        print(\"\\nMissing data matrix (Training Set):\")\n",
    "        msno.matrix(X_train)\n",
    "        plt.show()\n",
    "        print(\"\\nMissing data heatmap (Training Set):\")\n",
    "        msno.heatmap(X_train)\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"\\nInstall 'missingno' library for advanced missing data visualizations: pip install missingno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20a464",
   "metadata": {},
   "source": [
    "### 3.2 Imputation Strategies\n",
    "Define and apply imputation strategies based on the data type and amount of missingness.\n",
    "-   **Numerical Features:** Mean, Median (robust to outliers), or KNNImputer (considers relationships between features).\n",
    "-   **Categorical Features:** Mode (most frequent value) or a constant value like 'Missing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319afff",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns FOR IMPUTATION\n",
    "# Note: This might need adjustment based on actual dtypes after loading\n",
    "numerical_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category', 'boolean']).columns.tolist()\n",
    "\n",
    "logging.info(f\"Identified numerical columns: {numerical_cols}\")\n",
    "logging.info(f\"Identified categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Define imputation strategies (will be integrated into the pipeline later)\n",
    "# Example: Use Median for numerical, Mode for categorical\n",
    "# We define the transformers here but fit them within the pipeline\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# More advanced: KNNImputer for numerical (can be computationally expensive)\n",
    "# knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# --- Documentation of Decisions ---\n",
    "# Decision: Using Median for numerical features ('Annees_Experience', 'Indice_Richesse', etc.)\n",
    "#           as it's robust to potential outliers identified in EDA.\n",
    "# Decision: Using Mode (most frequent) for categorical features ('Niveau_Education',\n",
    "#           'Categorie_Socioprofessionnelle', 'Secteur_Activite', 'Possession_Voiture')\n",
    "#           as it's a simple and common approach for nominal/ordinal data.\n",
    "# Alternative Considered: KNNImputer for numerical data, but decided against it initially\n",
    "#                      due to potential computational cost and complexity. Can revisit if\n",
    "#                      SimpleImputer performance is suboptimal.\n",
    "# Alternative Considered: Creating a 'Missing' category for categorical features, but\n",
    "#                      Mode imputation is chosen for simplicity here.\n",
    "# ------------------------------------\n",
    "logging.info(\"Defined imputation strategies: Median for numerical, Mode for categorical.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19ead3",
   "metadata": {},
   "source": [
    "### 3.3 Validation (Conceptual - Applied within Pipeline)\n",
    "The actual imputation will happen within the scikit-learn pipeline to prevent data leakage. After the pipeline is fitted on `X_train`, we can transform both `X_train` and `X_test` and verify that there are no more missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414938b",
   "metadata": {},
   "source": [
    "## 4. Outlier Treatment\n",
    "\n",
    "### 4.1 Outlier Detection\n",
    "Identify outliers in numerical features using methods like the IQR (Interquartile Range) or Z-score. Visualization (box plots) is key here. We focus on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9903488",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"--- Outlier Detection (Training Set) ---\")\n",
    "\n",
    "# Focus on key numerical columns where outliers are expected/impactful\n",
    "# Example: 'Age', 'Annees_Experience', 'Nb_Personnes_Charge', 'Indice_Richesse'\n",
    "cols_to_check_outliers = [col for col in ['Age', 'Annees_Experience', 'Nb_Personnes_Charge', 'Indice_Richesse'] if col in numerical_cols]\n",
    "\n",
    "if not cols_to_check_outliers:\n",
    "    print(\"No numerical columns identified for outlier check.\")\n",
    "    logging.warning(\"No numerical columns found for outlier detection.\")\n",
    "else:\n",
    "    print(f\"Checking for outliers in: {cols_to_check_outliers}\")\n",
    "\n",
    "    # Calculate IQR bounds for each column\n",
    "    outlier_bounds = {}\n",
    "    for col in cols_to_check_outliers:\n",
    "        if X_train[col].isnull().any():\n",
    "             print(f\"Skipping outlier detection for '{col}' due to NaNs before imputation.\")\n",
    "             continue # Skip columns with NaNs for now, handle after imputation if needed\n",
    "\n",
    "        Q1 = X_train[col].quantile(0.25)\n",
    "        Q3 = X_train[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outlier_bounds[col] = (lower_bound, upper_bound)\n",
    "\n",
    "        outliers = X_train[(X_train[col] < lower_bound) | (X_train[col] > upper_bound)]\n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        print(f\"  IQR: {IQR:.2f}, Lower Bound: {lower_bound:.2f}, Upper Bound: {upper_bound:.2f}\")\n",
    "        print(f\"  Number of potential outliers detected: {len(outliers)}\")\n",
    "        # print(f\"  Outlier values (first 5): {outliers[col].head().tolist()}\") # Optional: print specific values\n",
    "\n",
    "    # Visualize using box plots BEFORE treatment\n",
    "    print(\"\\nBox plots BEFORE outlier treatment (Training Set):\")\n",
    "    plt.figure(figsize=(15, 5 * ((len(cols_to_check_outliers) + 1) // 2)))\n",
    "    for i, col in enumerate(cols_to_check_outliers):\n",
    "         if col in outlier_bounds: # Only plot if bounds were calculated\n",
    "            plt.subplot((len(cols_to_check_outliers) + 1) // 2, 2, i + 1)\n",
    "            sns.boxplot(x=X_train[col])\n",
    "            plt.title(f'Box Plot of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b3494d",
   "metadata": {},
   "source": [
    "### 4.2 Outlier Treatment Strategies\n",
    "Decide how to handle the detected outliers. Options include:\n",
    "1.  **Capping (Winsorization):** Limit extreme values to a certain percentile or the IQR bounds. Often preferred as it retains data.\n",
    "2.  **Transformation:** Apply mathematical transformations (e.g., log, sqrt) which can reduce the impact of outliers, especially in skewed data.\n",
    "3.  **Removal:** Delete rows with outliers. Use with caution, as it can lead to data loss.\n",
    "4.  **Imputation:** Treat outliers as missing values and impute them.\n",
    "\n",
    "**Decision:** We will use capping (Winsorization) based on the calculated IQR bounds for this example. This will be implemented via a custom transformer within the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb19f55",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- Documentation of Decisions ---\n",
    "# Decision: Apply capping (Winsorization) using the 1.5*IQR rule for features like\n",
    "#           'Annees_Experience' and 'Indice_Richesse' where extreme values might skew results\n",
    "#           but still contain valid information.\n",
    "# Decision: 'Age' outliers might be valid data points, so we might choose not to cap them,\n",
    "#           or use wider bounds (e.g., 3*IQR). For this example, we'll cap using 1.5*IQR.\n",
    "# Decision: 'Nb_Personnes_Charge' might have a natural upper limit, capping seems reasonable.\n",
    "# Reason: Capping retains the data points while mitigating the influence of extreme values,\n",
    "#         which is often preferable to removal. Transformation (like log) will be handled\n",
    "#         separately during feature scaling/normalization if needed for skewness.\n",
    "# ------------------------------------\n",
    "logging.info(f\"Outlier treatment strategy: Capping using 1.5*IQR bounds for {list(outlier_bounds.keys())}.\")\n",
    "\n",
    "# Define a custom transformer for capping (to be used in the pipeline)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Caps outliers in specified numerical columns using the IQR method.\n",
    "    Calculates bounds based on the training data.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None, factor=1.5):\n",
    "        self.columns = columns\n",
    "        self.factor = factor\n",
    "        self.bounds_ = {} # Store bounds learned from training data\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        if self.columns is None:\n",
    "            self.columns = X_.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "        for col in self.columns:\n",
    "            if col in X_.columns: # Check if column exists\n",
    "                # Handle potential NaNs before calculating quantiles\n",
    "                col_data = X_[col].dropna()\n",
    "                if not col_data.empty:\n",
    "                    Q1 = col_data.quantile(0.25)\n",
    "                    Q3 = col_data.quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - self.factor * IQR\n",
    "                    upper_bound = Q3 + self.factor * IQR\n",
    "                    self.bounds_[col] = (lower_bound, upper_bound)\n",
    "                    logging.debug(f\"OutlierCapper: Calculated bounds for {col}: ({lower_bound:.2f}, {upper_bound:.2f})\")\n",
    "                else:\n",
    "                    logging.warning(f\"OutlierCapper: Column '{col}' has only NaNs or is empty. Skipping.\")\n",
    "                    self.bounds_[col] = (None, None) # Store None if bounds can't be calculated\n",
    "            else:\n",
    "                 logging.warning(f\"OutlierCapper: Column '{col}' not found in DataFrame during fit.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        for col in self.columns:\n",
    "             if col in X_.columns and col in self.bounds_ and self.bounds_[col] != (None, None):\n",
    "                lower, upper = self.bounds_[col]\n",
    "                X_[col] = np.clip(X_[col], lower, upper)\n",
    "                logging.debug(f\"OutlierCapper: Applied capping to {col}.\")\n",
    "             elif col not in self.bounds_:\n",
    "                 logging.warning(f\"OutlierCapper: No bounds found for column '{col}' during transform. Skipping.\")\n",
    "             elif self.bounds_[col] == (None, None):\n",
    "                 logging.warning(f\"OutlierCapper: Skipping capping for column '{col}' as bounds could not be determined during fit.\")\n",
    "\n",
    "        return X_\n",
    "\n",
    "# Instantiate the capper for the columns identified\n",
    "capper = OutlierCapper(columns=list(outlier_bounds.keys()), factor=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff2560",
   "metadata": {},
   "source": [
    "### 4.3 Before/After Comparison (Conceptual - Visualized After Pipeline)\n",
    "We will visualize the box plots again *after* the entire pipeline (including capping) has been applied to the training data to confirm the effect of the treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7209ca",
   "metadata": {},
   "source": [
    "## 5. Feature Encoding and Transformation\n",
    "Convert categorical features into a numerical format suitable for ML algorithms.\n",
    "\n",
    "**Strategies:**\n",
    "* **One-Hot Encoding (OHE):** For nominal categorical features (no inherent order) like `Sexe`, `Milieu`, `Etat_Matrimonial`, `Categorie_Socioprofessionnelle`, `Secteur_Activite`. Creates binary columns for each category. Use `handle_unknown='ignore'` or `handle_unknown='infrequent_if_exist'` to manage categories present in test but not train.\n",
    "* **Ordinal Encoding:** For ordinal categorical features (inherent order) like `Niveau_Education`. Assigns integers based on order. Requires defining the order explicitly.\n",
    "* **Binary Encoding:** For binary features like `Possession_Voiture`, `Possession_Logement`, `Possession_Terrain`, `Acces_Internet`, `Education_Superieure`. Can map to 0/1. (OHE often handles this automatically, or `LabelEncoder` can be used carefully).\n",
    "\n",
    "**Moroccan Specifics:**\n",
    "* `Categorie_Socioprofessionnelle`, `Secteur_Activite`: Likely nominal, use OHE. Need to handle potentially large number of categories if applicable (e.g., using `max_categories` in OHE or grouping infrequent ones).\n",
    "* `Niveau_Education`: Clearly ordinal. Define the order carefully (e.g., Non scolarisé < Primaire < Secondaire < Universitaire).\n",
    "* `Milieu`: (Urbain/Rural) - Binary or OHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905cdbd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- Identify Columns for Each Encoding Type ---\n",
    "\n",
    "# Define the expected order for ordinal features\n",
    "# --- !!! ACTION REQUIRED !!! ---\n",
    "# Verify and adjust this order based on your specific data understanding\n",
    "education_order = ['Non scolarisé', 'Primaire', 'Secondaire', 'Universitaire']\n",
    "# Add other ordinal columns and their orders if any\n",
    "# Example: Categorie_Age might be ordinal if defined like 'Jeune', 'Adulte', 'Senior'\n",
    "# categorie_age_order = ['Jeune Adulte', 'Adulte', 'Senior'] # Example\n",
    "\n",
    "# Identify columns for each strategy (use columns present in X_train)\n",
    "all_categorical_cols = X_train.select_dtypes(include=['object', 'category', 'boolean']).columns.tolist()\n",
    "\n",
    "# --- !!! ACTION REQUIRED !!! ---\n",
    "# Assign columns to the correct encoding strategy based on your data\n",
    "ordinal_cols = [col for col in ['Niveau_Education'] if col in all_categorical_cols] # Add 'Categorie_Age' if ordinal\n",
    "ohe_cols = [col for col in all_categorical_cols if col not in ordinal_cols]\n",
    "\n",
    "# Binary columns might already be boolean or can be handled by OHE.\n",
    "# If they are object type ('Oui'/'Non', 'Vrai'/'Faux'), they need specific mapping or will be handled by OHE.\n",
    "# Let's assume boolean columns are handled correctly or fall into OHE for now.\n",
    "\n",
    "logging.info(f\"Columns identified for One-Hot Encoding: {ohe_cols}\")\n",
    "logging.info(f\"Columns identified for Ordinal Encoding: {ordinal_cols}\")\n",
    "# Note: Binary features (True/False) are often handled implicitly by downstream tasks or can be included in OHE.\n",
    "\n",
    "# --- Define Encoders (for Pipeline) ---\n",
    "# Ordinal Encoder: Specify categories and handle potential unknown values\n",
    "# Note: SimpleImputer (mode) should run BEFORE OrdinalEncoder if NaNs exist\n",
    "ordinal_encoder = OrdinalEncoder(\n",
    "    categories=[education_order] * len(ordinal_cols), # Adjust if multiple ordinal cols have different orders\n",
    "    handle_unknown='use_encoded_value', # Assign a specific value (e.g., -1) to unknowns\n",
    "    unknown_value=-1 # Or np.nan\n",
    ")\n",
    "\n",
    "# One-Hot Encoder: Drop first category to avoid multicollinearity, handle unknowns\n",
    "one_hot_encoder = OneHotEncoder(\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore' # Ignores categories in test set not seen in training set\n",
    "    # Consider 'infrequent_if_exist' for grouping rare categories\n",
    ")\n",
    "\n",
    "# --- Documentation of Decisions ---\n",
    "# Decision: Using One-Hot Encoding for nominal features like 'Sexe', 'Milieu', 'Etat_Matrimonial', etc.\n",
    "#           Chosen 'handle_unknown=ignore' for simplicity, assuming categories missing in train\n",
    "#           but present in test are rare or okay to ignore.\n",
    "# Decision: Using Ordinal Encoding for 'Niveau_Education' based on the defined educational hierarchy.\n",
    "#           Handling unknown values by assigning -1.\n",
    "# Decision: Boolean features ('Possession_Voiture', etc.) will be implicitly handled by OHE\n",
    "#           if they are object type, or passed through if already boolean/numeric (0/1).\n",
    "#           If imputation filled NaNs with a string like 'Missing', OHE will handle that too.\n",
    "# ------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fe6d5",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling and Normalization\n",
    "Scale numerical features to a common range.\n",
    "\n",
    "**Strategies:**\n",
    "* **Standardization (`StandardScaler`):** Scales data to have zero mean and unit variance (Z-score normalization). Good default choice, less affected by outliers than MinMaxScaler.\n",
    "* **Normalization (`MinMaxScaler`):** Scales data to a fixed range, usually [0, 1]. Sensitive to outliers. Useful if the algorithm requires data in a specific range (e.g., some neural networks).\n",
    "* **Transformation (`PowerTransformer`):** Apply transformations like Box-Cox (positive data only) or Yeo-Johnson (positive and negative data) to make data more Gaussian-like. Useful for skewed distributions (like `Revenu_Annuel`, although we scale features X here).\n",
    "\n",
    "**Decision:** Use `StandardScaler` for most numerical features. Explore `PowerTransformer` for highly skewed features identified in EDA (e.g., `Annees_Experience`, `Indice_Richesse` if skewed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33346d21",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- Identify Skewed Features (from EDA or check now) ---\n",
    "print(\"--- Skewness Check (Training Set - Numerical Columns) ---\")\n",
    "# We need imputed data to check skewness accurately.\n",
    "# For now, let's assume EDA identified skewed columns.\n",
    "# --- !!! ACTION REQUIRED !!! ---\n",
    "# Update this list based on your EDA findings or a check after imputation.\n",
    "skewed_cols = [col for col in ['Annees_Experience', 'Indice_Richesse'] if col in numerical_cols]\n",
    "logging.info(f\"Columns identified as potentially skewed: {skewed_cols}\")\n",
    "\n",
    "# --- Define Scalers (for Pipeline) ---\n",
    "standard_scaler = StandardScaler()\n",
    "power_transformer = PowerTransformer(method='yeo-johnson') # Yeo-Johnson handles 0s and negative values\n",
    "\n",
    "# --- Documentation of Decisions ---\n",
    "# Decision: Apply StandardScaler to all numerical features *after* imputation and capping.\n",
    "#           This ensures features have zero mean and unit variance.\n",
    "# Decision: Additionally, consider applying PowerTransformer (Yeo-Johnson) *before* StandardScaler\n",
    "#           to skewed features like 'Annees_Experience', 'Indice_Richesse' to make their\n",
    "#           distribution more Gaussian, potentially improving model performance.\n",
    "#           For simplicity in the initial pipeline, we might start with only StandardScaler.\n",
    "#           Let's include PowerTransformer in the plan but maybe comment it out initially.\n",
    "# Alternative Considered: MinMaxScaler, but StandardScaler is generally more robust to outliers\n",
    "#                      that might remain even after capping.\n",
    "# ------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eaaaac",
   "metadata": {},
   "source": [
    "### 6.1 Before/After Distribution Comparison (Conceptual - Visualized After Pipeline)\n",
    "After applying the pipeline, we will visualize the distributions (e.g., histograms or KDE plots) of numerical features before and after scaling/transformation to verify the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e6a9c",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering (Optional but Recommended)\n",
    "Create new features that might capture more complex patterns or domain knowledge.\n",
    "\n",
    "**Examples:**\n",
    "* **Interaction Terms:** Combine two or more features (e.g., `Age` * `Annees_Experience`).\n",
    "* **Polynomial Features:** Create polynomial terms (e.g., `Age`^2, `Annees_Experience`^2). Use `sklearn.preprocessing.PolynomialFeatures`.\n",
    "* **Domain-Specific Features:** Create features based on Moroccan context (e.g., interaction between `Milieu` and `Niveau_Education`, or `Categorie_Socioprofessionnelle` and `Secteur_Activite`). Requires careful thought.\n",
    "* **Binning/Discretization:** Convert continuous features like `Age` or `Annees_Experience` into categorical bins (e.g., 'Young', 'Mid-Career', 'Experienced').\n",
    "\n",
    "**Implementation:** This often requires custom transformers or functions applied *before* scaling/encoding if the engineered features are numerical/categorical respectively, or *after* if combining already processed features. It's often an iterative process done alongside modeling. For this initial pipeline, we might add simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35e471",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- Example: Binning Age ---\n",
    "# This could be done via a custom transformer or KBinsDiscretizer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Example: Create age bins\n",
    "# This should ideally be defined based on EDA/domain knowledge\n",
    "age_binner = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile', subsample=None) # Using quantile strategy\n",
    "\n",
    "# --- Example: Interaction Term (Conceptual) ---\n",
    "# Could create 'Age_x_Experience'\n",
    "# Often done using PolynomialFeatures(interaction_only=True) or custom function\n",
    "\n",
    "# --- Documentation of Decisions ---\n",
    "# Decision: Add Age binning using KBinsDiscretizer as an example of feature engineering.\n",
    "#           This converts 'Age' into ordinal categories.\n",
    "# Decision: Keep feature engineering minimal in the initial pipeline for clarity.\n",
    "#           More complex interactions or polynomial features can be added later based on\n",
    "#           model performance and feature importance analysis.\n",
    "# Note: Feature engineering steps need careful placement within the main pipeline\n",
    "#       (e.g., binning numerical before scaling, interactions might be before or after).\n",
    "# ------------------------------------\n",
    "logging.info(\"Defined Age binning as an example feature engineering step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738a37a",
   "metadata": {},
   "source": [
    "## 8. Dimensionality Assessment (Optional)\n",
    "Analyze the relationships between features, especially after engineering new ones.\n",
    "\n",
    "**Techniques:**\n",
    "* **Correlation Analysis:** Check correlation between numerical features (using a heatmap). High correlation (> 0.8 or < -0.8) might indicate redundancy.\n",
    "* **Multicollinearity Check (VIF):** Variance Inflation Factor (VIF) can quantify how much a feature's variance is inflated by other features. VIF > 5 or 10 often indicates problematic multicollinearity. Requires statsmodels library.\n",
    "* **Basic Feature Selection:** Techniques like selecting features with high correlation to the target, or low correlation among themselves. Often done more formally during modeling (e.g., using model-based feature importance, RFE).\n",
    "* **Principal Component Analysis (PCA):** A dimensionality reduction technique that transforms features into a smaller set of uncorrelated components. Can be useful if multicollinearity is high or if dealing with a very large number of features. Apply *after* scaling.\n",
    "\n",
    "**Implementation:** Correlation/VIF checks are usually done *after* initial preprocessing on the transformed training data. PCA can be added as a final step in the pipeline if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc95ca2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- Correlation Analysis (Conceptual - Run After Pipeline Transformation) ---\n",
    "# Placeholder for code to run after pipeline transforms X_train_processed\n",
    "# Example:\n",
    "# corr_matrix = X_train_processed.corr()\n",
    "# sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
    "# plt.title('Correlation Matrix of Processed Features')\n",
    "# plt.show()\n",
    "\n",
    "# --- VIF Check (Conceptual - Run After Pipeline Transformation) ---\n",
    "# Placeholder for code to run after pipeline transforms X_train_processed\n",
    "# Requires statsmodels: pip install statsmodels\n",
    "# Example:\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# vif_data = pd.DataFrame()\n",
    "# vif_data[\"feature\"] = X_train_processed.columns\n",
    "# vif_data[\"VIF\"] = [variance_inflation_factor(X_train_processed.values, i) for i in range(X_train_processed.shape[1])]\n",
    "# print(vif_data.sort_values('VIF', ascending=False))\n",
    "\n",
    "# --- PCA Exploration (Conceptual - Can be added to Pipeline) ---\n",
    "pca_explorer = PCA(n_components=0.95) # Retain 95% of variance\n",
    "# Can fit this on scaled data to see how many components are needed.\n",
    "\n",
    "# --- Documentation of Decisions ---\n",
    "# Decision: Defer detailed correlation and VIF analysis until *after* the pipeline is built\n",
    "#           and applied to the training data, as transformations affect these metrics.\n",
    "# Decision: Do not include PCA in the initial pipeline by default. It can be added later\n",
    "#           if dimensionality reduction is deemed necessary based on the number of features\n",
    "#           after OHE, multicollinearity analysis, or model performance.\n",
    "# ------------------------------------\n",
    "logging.info(\"Dimensionality assessment (correlation, VIF) planned after pipeline transformation. PCA not included initially.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad80d4",
   "metadata": {},
   "source": [
    "## 9. Establish Preprocessing Pipeline\n",
    "Combine all the chosen steps into a single `scikit-learn` `Pipeline` using `ColumnTransformer` to apply different steps to different columns. This ensures consistency and prevents data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a84c2b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- Re-identify column types based on the original training dataframe (X_train) ---\n",
    "# This is crucial for ColumnTransformer\n",
    "numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category', 'boolean']).columns.tolist()\n",
    "\n",
    "# Adjust lists based on specific encoding needs identified earlier\n",
    "ordinal_features = [col for col in ['Niveau_Education'] if col in categorical_features] # Add others if needed\n",
    "ohe_features = [col for col in categorical_features if col not in ordinal_features]\n",
    "\n",
    "# Ensure features for capping/skewness are in numerical_features\n",
    "outlier_cap_features = [col for col in list(outlier_bounds.keys()) if col in numerical_features]\n",
    "skewed_features = [col for col in ['Annees_Experience', 'Indice_Richesse'] if col in numerical_features] # From step 6\n",
    "non_skewed_num_features = [col for col in numerical_features if col not in skewed_features]\n",
    "\n",
    "\n",
    "# --- Define Pipelines for different transformations ---\n",
    "\n",
    "# Pipeline for numerical features identified as skewed\n",
    "# 1. Impute missing values (median)\n",
    "# 2. Cap outliers (if applicable to skewed features)\n",
    "# 3. Apply Yeo-Johnson transformation\n",
    "# 4. Scale (StandardScaler)\n",
    "skewed_numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('capper', OutlierCapper(columns= [col for col in outlier_cap_features if col in skewed_features], factor=1.5)), # Cap only skewed cols that need it\n",
    "    ('transformer', PowerTransformer(method='yeo-johnson')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for other numerical features\n",
    "# 1. Impute missing values (median)\n",
    "# 2. Cap outliers (if applicable to non-skewed features)\n",
    "# 3. Scale (StandardScaler)\n",
    "non_skewed_numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('capper', OutlierCapper(columns= [col for col in outlier_cap_features if col not in skewed_features], factor=1.5)), # Cap only non-skewed cols that need it\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "# Pipeline for ordinal categorical features\n",
    "# 1. Impute missing values (most frequent)\n",
    "# 2. Ordinal encode\n",
    "ordinal_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(\n",
    "        categories=[education_order], # Adjust if multiple ordinal features\n",
    "        handle_unknown='use_encoded_value',\n",
    "        unknown_value=-1 # Or np.nan\n",
    "        ))\n",
    "])\n",
    "\n",
    "# Pipeline for nominal categorical features (One-Hot Encode)\n",
    "# 1. Impute missing values (most frequent or a constant like 'Missing')\n",
    "# 2. One-Hot encode\n",
    "ohe_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Or strategy='constant', fill_value='Missing'\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# --- Combine pipelines using ColumnTransformer ---\n",
    "# Note: Ensure column lists are accurate and cover all necessary features.\n",
    "#       Features not explicitly listed will be dropped by default, unless remainder='passthrough'.\n",
    "\n",
    "# Create the list of transformers for ColumnTransformer\n",
    "transformers_list = []\n",
    "\n",
    "if non_skewed_num_features:\n",
    "    transformers_list.append(('num_non_skewed', non_skewed_numerical_pipeline, non_skewed_num_features))\n",
    "    logging.info(f\"Added non-skewed numerical pipeline for: {non_skewed_num_features}\")\n",
    "if skewed_features:\n",
    "     transformers_list.append(('num_skewed', skewed_numerical_pipeline, skewed_features))\n",
    "     logging.info(f\"Added skewed numerical pipeline for: {skewed_features}\")\n",
    "if ordinal_features:\n",
    "    transformers_list.append(('ordinal', ordinal_pipeline, ordinal_features))\n",
    "    logging.info(f\"Added ordinal pipeline for: {ordinal_features}\")\n",
    "if ohe_features:\n",
    "    transformers_list.append(('onehot', ohe_pipeline, ohe_features))\n",
    "    logging.info(f\"Added one-hot encoding pipeline for: {ohe_features}\")\n",
    "\n",
    "# Add Feature Engineering Steps (Example: Age Binning)\n",
    "# If binning 'Age', it should replace 'Age' in the numerical pipelines above\n",
    "# This requires more careful integration, potentially running KBinsDiscretizer first\n",
    "# or creating a more complex ColumnTransformer structure.\n",
    "\n",
    "# For simplicity here, let's assume we handle 'Age' normally first,\n",
    "# and binning could be explored separately or integrated carefully.\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=transformers_list,\n",
    "    remainder='passthrough' # Keep columns not specified (if any - should be none ideally)\n",
    "                            # Change to 'drop' if you want to be strict\n",
    ")\n",
    "\n",
    "# --- Create the Full Preprocessing Pipeline ---\n",
    "# This can include the preprocessor and potentially later steps like PCA if desired\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "    # Optional: ('pca', PCA(n_components=0.95)) # Add PCA here if needed\n",
    "])\n",
    "\n",
    "logging.info(\"Full preprocessing pipeline created using ColumnTransformer.\")\n",
    "print(\"\\nFull Preprocessing Pipeline Structure:\")\n",
    "print(full_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044052c",
   "metadata": {},
   "source": [
    "### 9.1 Fit the Pipeline on Training Data\n",
    "Fit the entire pipeline on the training data (`X_train`). This learns the imputation values, scaling parameters, encoding mappings, etc., *only* from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a61e86",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Fit the pipeline to the training data\n",
    "    full_pipeline.fit(X_train, y_train) # y_train might be needed if pipeline includes steps requiring it (e.g., some feature selection)\n",
    "    logging.info(\"Preprocessing pipeline fitted successfully on the training data.\")\n",
    "\n",
    "    # Apply the pipeline to transform training and test sets\n",
    "    X_train_processed = full_pipeline.transform(X_train)\n",
    "    X_test_processed = full_pipeline.transform(X_test)\n",
    "    logging.info(\"Training and testing data transformed using the fitted pipeline.\")\n",
    "\n",
    "    # Get feature names after transformation (important!)\n",
    "    # This requires scikit-learn version that supports get_feature_names_out\n",
    "    try:\n",
    "        feature_names_out = full_pipeline.get_feature_names_out()\n",
    "        logging.info(f\"Generated {len(feature_names_out)} feature names after transformation.\")\n",
    "        # Convert processed arrays back to DataFrames (optional but good for inspection)\n",
    "        X_train_processed_df = pd.DataFrame(X_train_processed, columns=feature_names_out, index=X_train.index)\n",
    "        X_test_processed_df = pd.DataFrame(X_test_processed, columns=feature_names_out, index=X_test.index)\n",
    "        print(\"\\nShape of processed training data:\", X_train_processed_df.shape)\n",
    "        print(\"Shape of processed test data:\", X_test_processed_df.shape)\n",
    "        print(\"\\nFirst 5 rows of processed training data:\")\n",
    "        display(X_train_processed_df.head())\n",
    "\n",
    "        # --- Validation Step: Check for NaNs after processing ---\n",
    "        print(\"\\nChecking for NaNs in processed data:\")\n",
    "        print(f\"  NaNs in X_train_processed: {np.isnan(X_train_processed).sum()}\")\n",
    "        print(f\"  NaNs in X_test_processed: {np.isnan(X_test_processed).sum()}\")\n",
    "        if np.isnan(X_train_processed).sum() == 0 and np.isnan(X_test_processed).sum() == 0:\n",
    "            logging.info(\"Validation successful: No NaNs found in processed data.\")\n",
    "        else:\n",
    "            logging.error(\"Validation failed: NaNs found in processed data. Review pipeline steps.\")\n",
    "\n",
    "\n",
    "    except AttributeError:\n",
    "        logging.warning(\"get_feature_names_out not available in this scikit-learn version. Processed data is numpy array.\")\n",
    "        # Handle case for older scikit-learn versions (feature names are harder to get)\n",
    "        X_train_processed_df = pd.DataFrame(X_train_processed, index=X_train.index)\n",
    "        X_test_processed_df = pd.DataFrame(X_test_processed, index=X_test.index)\n",
    "        print(\"\\nShape of processed training data (Numpy Array):\", X_train_processed.shape)\n",
    "        print(\"Shape of processed test data (Numpy Array):\", X_test_processed.shape)\n",
    "        # Cannot display head easily without column names\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during pipeline fitting or transformation: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05588413",
   "metadata": {},
   "source": [
    "### 9.2 Visualization After Processing (Outliers and Distributions)\n",
    "Now visualize the data *after* the pipeline has been applied to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca9fb82",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Check if processed data is available as DataFrame\n",
    "if isinstance(X_train_processed_df, pd.DataFrame):\n",
    "    print(\"\\n--- Visualizations After Preprocessing (Training Set) ---\")\n",
    "\n",
    "    # --- Box Plots After Outlier Treatment ---\n",
    "    # Identify columns in the processed data that correspond to the original capped columns\n",
    "    # This requires careful mapping based on get_feature_names_out or pipeline structure\n",
    "    # Example: If 'Annees_Experience' was capped and scaled in 'num_non_skewed' pipeline\n",
    "    processed_cols_to_check = []\n",
    "    original_capped_cols = list(outlier_bounds.keys()) # From step 4.1\n",
    "\n",
    "    # Attempt to find the corresponding processed columns\n",
    "    if 'feature_names_out' in locals():\n",
    "        for col in original_capped_cols:\n",
    "            # Find feature names starting with the transformer prefix and original column name\n",
    "            # Example: 'num_non_skewed__Annees_Experience' or 'num_skewed__Indice_Richesse'\n",
    "            matching_cols = [fn for fn in feature_names_out if f\"__{col}\" in fn]\n",
    "            if matching_cols:\n",
    "                processed_cols_to_check.extend(matching_cols)\n",
    "\n",
    "    if processed_cols_to_check:\n",
    "        print(\"\\nBox plots AFTER outlier treatment/scaling (Processed Training Set):\")\n",
    "        plt.figure(figsize=(15, 5 * ((len(processed_cols_to_check) + 1) // 2)))\n",
    "        for i, col in enumerate(processed_cols_to_check):\n",
    "            plt.subplot((len(processed_cols_to_check) + 1) // 2, 2, i + 1)\n",
    "            sns.boxplot(x=X_train_processed_df[col])\n",
    "            plt.title(f'Box Plot of {col} (Processed)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nCould not automatically identify processed columns for outlier comparison.\")\n",
    "\n",
    "\n",
    "    # --- Distributions After Scaling/Transformation ---\n",
    "    # Identify columns corresponding to original numerical features\n",
    "    processed_num_cols = []\n",
    "    original_num_cols = numerical_features # From start of section 9\n",
    "\n",
    "    if 'feature_names_out' in locals():\n",
    "        for col in original_num_cols:\n",
    "             matching_cols = [fn for fn in feature_names_out if f\"__{col}\" in fn]\n",
    "             if matching_cols:\n",
    "                 processed_num_cols.extend(matching_cols)\n",
    "\n",
    "    if processed_num_cols:\n",
    "        print(\"\\nDistributions AFTER scaling/transformation (Processed Training Set):\")\n",
    "        X_train_processed_df[processed_num_cols].hist(figsize=(15, 10), bins=30)\n",
    "        plt.suptitle(\"Histograms of Processed Numerical Features\")\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nCould not automatically identify processed numerical columns for distribution plots.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping post-processing visualizations as processed data is not in a DataFrame (likely due to older scikit-learn version).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1c461",
   "metadata": {},
   "source": [
    "### 9.3 Serialize the Pipeline\n",
    "Save the fitted pipeline object to a file using `pickle`. This allows you to load and reuse the exact same preprocessing steps on new data later (e.g., for predictions) without refitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3f6a8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define file path for saving the pipeline\n",
    "PIPELINE_PATH = 'moroccan_income_preprocessor.pkl'\n",
    "\n",
    "try:\n",
    "    with open(PIPELINE_PATH, 'wb') as f:\n",
    "        pickle.dump(full_pipeline, f)\n",
    "    logging.info(f\"Preprocessing pipeline successfully saved to {PIPELINE_PATH}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving pipeline: {e}\")\n",
    "\n",
    "# Example of loading the pipeline later:\n",
    "# with open(PIPELINE_PATH, 'rb') as f:\n",
    "#     loaded_pipeline = pickle.load(f)\n",
    "# new_data_processed = loaded_pipeline.transform(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f4ae0",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Export\n",
    "\n",
    "### 10.1 Export Processed Datasets\n",
    "Save the processed training and testing sets (features and target) to new files (e.g., CSV or Parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2215949",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define output file paths\n",
    "TRAIN_FEATURES_PATH = 'train_features_processed.csv'\n",
    "TRAIN_TARGET_PATH = 'train_target.csv'\n",
    "TEST_FEATURES_PATH = 'test_features_processed.csv'\n",
    "TEST_TARGET_PATH = 'test_target.csv'\n",
    "\n",
    "try:\n",
    "    # Save features (use DataFrame if available, otherwise numpy array)\n",
    "    if isinstance(X_train_processed_df, pd.DataFrame):\n",
    "        X_train_processed_df.to_csv(TRAIN_FEATURES_PATH, index=False)\n",
    "        X_test_processed_df.to_csv(TEST_FEATURES_PATH, index=False)\n",
    "    else:\n",
    "        # Save numpy arrays (consider adding header manually if needed)\n",
    "        np.savetxt(TRAIN_FEATURES_PATH, X_train_processed, delimiter=',')\n",
    "        np.savetxt(TEST_FEATURES_PATH, X_test_processed, delimiter=',')\n",
    "\n",
    "    # Save target variable (Series)\n",
    "    y_train.to_csv(TRAIN_TARGET_PATH, index=False, header=True)\n",
    "    y_test.to_csv(TEST_TARGET_PATH, index=False, header=True)\n",
    "\n",
    "    logging.info(f\"Processed training features saved to {TRAIN_FEATURES_PATH}\")\n",
    "    logging.info(f\"Training target saved to {TRAIN_TARGET_PATH}\")\n",
    "    logging.info(f\"Processed testing features saved to {TEST_FEATURES_PATH}\")\n",
    "    logging.info(f\"Testing target saved to {TEST_TARGET_PATH}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error exporting processed data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c365e34b",
   "metadata": {},
   "source": [
    "### 10.2 Summary of Transformations\n",
    "* **Data Splitting:** Data split into 80% training and 20% testing sets.\n",
    "* **Irrelevant Columns Dropped:** `ID`, `ID_Aleatoire`, `Code_Postal`, `Couleur_Preferee`, `Age_Mois`.\n",
    "* **Missing Value Imputation:** Numerical features imputed using median; Categorical features imputed using mode.\n",
    "* **Outlier Treatment:** Outliers in `Age`, `Annees_Experience`, `Nb_Personnes_Charge`, `Indice_Richesse` (example columns) were capped using the 1.5*IQR method.\n",
    "* **Encoding:** Ordinal features (`Niveau_Education`) encoded using `OrdinalEncoder`; Nominal features (`Sexe`, `Milieu`, `Etat_Matrimonial`, etc.) encoded using `OneHotEncoder`.\n",
    "* **Scaling/Transformation:** Skewed numerical features (`Annees_Experience`, `Indice_Richesse`) transformed using Yeo-Johnson `PowerTransformer` and then scaled using `StandardScaler`. Other numerical features scaled using `StandardScaler`.\n",
    "* **Feature Engineering:** (Example) `Age` potentially binned using `KBinsDiscretizer` (adjust pipeline if implemented).\n",
    "* **Pipeline:** All steps encapsulated in a `scikit-learn` `Pipeline` using `ColumnTransformer` for reproducibility and consistency.\n",
    "\n",
    "### 10.3 Quality Metrics (Before vs. After)\n",
    "* **Missing Values:** Reduced from X% in raw training features to 0% in processed training features.\n",
    "* **Data Types:** All features converted to numerical types suitable for modeling.\n",
    "* **Scale:** Numerical features standardized (mean ~0, std dev ~1) or normalized.\n",
    "* **Outliers:** Extreme values mitigated through capping.\n",
    "* **Dimensionality:** Changed from Y original features to Z features after OHE (check `X_train_processed_df.shape[1]`).\n",
    "\n",
    "### 10.4 Documentation\n",
    "This notebook serves as the primary documentation for the preprocessing workflow. The saved pipeline (`moroccan_income_preprocessor.pkl`) encapsulates the process for reuse. Key decisions and justifications are documented in markdown cells throughout the notebook.\n",
    "\n",
    "### 10.5 Preparation for Modeling\n",
    "The exported files (`train_features_processed.csv`, `train_target.csv`, `test_features_processed.csv`, `test_target.csv`) contain the clean, processed data ready to be used for training and evaluating machine learning models for Moroccan income prediction. The saved pipeline can be loaded in the modeling phase to process any new data consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa1d34",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logging.info(\"--- End of Preprocessing Notebook ---\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
